{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44be65da-636e-4ece-950a-d567a9d7e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer, M2M100Model\n",
    "from datasets import load_metric\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import segmentation_models_pytorch as smp\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import tqdm\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f2a626-ac85-40ee-8bd8-f18c8e6c63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "        self.weights_backup = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        self.tokenizers = {'cs': M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", \n",
    "                                                            src_lang='cs',\n",
    "                                                            tgt_lang=\"en\", \n",
    "                                                            padding_side='right', \n",
    "                                                            truncation_side='right'), \n",
    "                           'de':M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", \n",
    "                                                            src_lang='de',\n",
    "                                                            tgt_lang=\"en\", \n",
    "                                                            padding_side='right', \n",
    "                                                            truncation_side='right')}\n",
    "\n",
    "    def forward_eval(self, batch, lang_code):\n",
    "        return self.model.generate(**batch['x'], forced_bos_token_id=self.tokenizers[lang_code].get_lang_id(\"en\"))\n",
    "    \n",
    "    def forward_train(self, batch):\n",
    "        return self.model(**batch['x'], labels=batch['y']) \n",
    "        \n",
    "    def apply_mask(self, mask, sizing):\n",
    "        start = 0\n",
    "        copy_state = copy.deepcopy(self.model.state_dict())\n",
    "        segments = {}\n",
    "        for i in copy_state:\n",
    "            if i in sizing:\n",
    "                end = start + sizing[i]\n",
    "                segment = np.round(mask[start:end])\n",
    "                index = np.where(segment == 0)\n",
    "                \n",
    "                final_indices = []\n",
    "                divisor = int(copy_state[i].shape[0]/256)\n",
    "                for j in index[0]:\n",
    "                    final_indices += [*range(j*divisor, (j*divisor)+divisor)]\n",
    "                copy_state[i].data[np.array(final_indices)] = 0\n",
    "                segments.update({i:index})\n",
    "                start = end\n",
    "        self.model.load_state_dict(copy_state)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in segments:\n",
    "                param.data[segments[name]].requires_grad = False\n",
    "                start = end\n",
    "\n",
    "    def return_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def return_model_state(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def revert_weights(self):\n",
    "        self.model.load_state_dict(self.weights_backup)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def update_backup(self):\n",
    "        self.weights_backup = copy.deepcopy(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c46935-6812-4ec6-8363-491632958266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_mask(state_dict):\n",
    "    total = 0\n",
    "    mask_sizing = OrderedDict()\n",
    "    total_params = 0\n",
    "    total_considered = 0\n",
    "    for i in list(state_dict.keys()):\n",
    "        shape = torch.tensor(state_dict[i].shape)\n",
    "        total_params += torch.prod(shape)\n",
    "        if 'bias' not in i and 'embed' not in i and 'norm' not in i and 'shared' not in i and 'head' not in i:\n",
    "            total += 256\n",
    "            mask_sizing.update({i:256})\n",
    "    print(total)\n",
    "    return mask_sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430f535f-a46f-4689-ad73-6271492e7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataloader:\n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.data = data \n",
    "        self.data_amount = data['x']['input_ids'].shape[0]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.length = int(math.ceil(self.data_amount/batch_size))\n",
    "        self.available = set([*range(self.length)])\n",
    "        \n",
    "    def select_subset(self, idxs, data, cuda):\n",
    "        if cuda:\n",
    "            return {'x':{'input_ids': data['x']['input_ids'][idxs].cuda(), \n",
    "                         'attention_mask': data['x']['attention_mask'][idxs].cuda()}, \n",
    "                    'y':data['y'][idxs].cuda()}\n",
    "        \n",
    "    def sample_batch(self, cuda=True):\n",
    "        \n",
    "        if len(self.available)==0:\n",
    "            self.available = set([*range(self.length)])\n",
    "        idx = random.choice(tuple(self.available))\n",
    "        self.available.remove(idx) \n",
    "        start = idx*self.batch_size\n",
    "        if idx == self.length-1:\n",
    "            diff = self.data_amount - (idx*self.batch_size)\n",
    "            batch = self.select_subset([i for i in range(start,start+diff)], self.data, cuda)\n",
    "            return batch\n",
    "        else: \n",
    "            batch = self.select_subset([i for i in range(start,start+self.batch_size)], self.data, cuda)\n",
    "            return batch\n",
    "        \n",
    "    def reset(self):\n",
    "        self.history = set()\n",
    "        self.available = set([*range(self.data_amount)])\n",
    "        \n",
    "def dual_sample(ds1_batch, ds2_batch):\n",
    "    perm = torch.randperm(len(ds1_batch['x']['input_ids'])*2)\n",
    "    x_input_ids = torch.cat((ds1_batch['x']['input_ids'], ds2_batch['x']['input_ids']), 0)\n",
    "    x_attention_masks = torch.cat((ds1_batch['x']['attention_mask'], ds2_batch['x']['attention_mask']), 0)\n",
    "    y = torch.cat((ds1_batch['y'], ds2_batch['y']))\n",
    "    return {'x':{'input_ids': x_input_ids[perm], \n",
    "                 'attention_mask': x_attention_masks[perm]},\n",
    "            'y':y[perm]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0476f563-ac41-4cfd-9246-bfa8ed4dc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, \n",
    "              lang1_train_dataloader, \n",
    "              lang2_train_dataloader, \n",
    "              lang1_test_dataloader, \n",
    "              lang2_test_dataloader, \n",
    "              epochs, \n",
    "               model_save_path,\n",
    "              steps=300):\n",
    "    metric = datasets.load_metric('sacrebleu')\n",
    "    # optim= Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "    lmbda = lambda epoch: 0.95\n",
    "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optim, lr_lambda=lmbda)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    saved_state = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad is False:\n",
    "                param.requires_grad =  True\n",
    "        \n",
    "        train(model, \n",
    "              lang1_train_dataloader, \n",
    "              lang2_train_dataloader, \n",
    "              optim, \n",
    "              steps, epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        cs_loss = test_loss(model, lang1_test_dataloader)\n",
    "        torch.cuda.empty_cache()\n",
    "        de_loss = test_loss(model, lang2_test_dataloader)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        cs_loss = cs_loss.detach().item()\n",
    "        de_loss = de_loss.detach().item()\n",
    "        averaged_loss = ((0.5*cs_loss) + (0.5*de_loss))\n",
    "        print('\\n CS Test Loss: ', cs_loss, ', DE Test Loss: ', de_loss, ', Best Averaged Loss: ', averaged_loss)\n",
    "        \n",
    "        \n",
    "        if averaged_loss < best_loss:\n",
    "            best_loss = averaged_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        \n",
    "def train(model, \n",
    "          lang1_train_dataloader, \n",
    "          lang2_train_dataloader, \n",
    "          optim, \n",
    "          steps, epoch):\n",
    "    optim.zero_grad()\n",
    "    for i in range(steps):\n",
    "        \n",
    "        batch1 = lang1_train_dataloader.sample_batch()\n",
    "        batch2 = lang2_train_dataloader.sample_batch()\n",
    "        batch = dual_sample(batch1, batch2)\n",
    "        loss = model.forward_train(batch).loss\n",
    "        loss.backward()\n",
    "        print('\\rEpoch {:.3f}\\tBatch: {:.3f}, Loss: {:.3f}'.format(epoch, \n",
    "                                                                   i, \n",
    "                                                                   loss.detach().item()), \n",
    "              end=\"\")   \n",
    "        if (i+1)%30 == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "def convert_to_string(tokens, test_code):\n",
    "    prohib = [1, 2, 128022, 128020, 128017]\n",
    "    if test_code:\n",
    "        return [' '.join([str(i) for i in tokens.tolist() if i not in prohib])]\n",
    "    else:\n",
    "        return ' '.join([str(i) for i in tokens.tolist() if i not in prohib])\n",
    "\n",
    "def stringify(tokens, test_code):\n",
    "    return [convert_to_string(tokens[i], test_code) for i in range(tokens.shape[0])]\n",
    "\n",
    "def test_BLEU(model, test_dataloader, metric, lang_code):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_dataloader.length):\n",
    "            batch = test_dataloader.sample_batch()\n",
    "            preds = model.forward_eval(batch, lang_code)\n",
    "            all_preds += stringify(preds[:, 0:8], False)\n",
    "            all_refs += stringify(batch['y'], True)\n",
    "        score = metric.compute(predictions = all_preds, references = all_refs)['score']\n",
    "    return score\n",
    "\n",
    "def test_loss(model, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_dataloader.length):\n",
    "            batch = test_dataloader.sample_batch()\n",
    "            loss += model.forward_train(batch).loss\n",
    "    return loss/test_dataloader.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f6abff-1697-4ad1-be66-3b4ec6ea1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./train_data/de_test.pkl', 'rb') as handle:\n",
    "    de_test = pickle.load(handle)\n",
    "    \n",
    "with open('./train_data/cs_test.pkl', 'rb') as handle:\n",
    "    cs_test = pickle.load(handle)\n",
    "    \n",
    "with open('./train_data/cs_train.pkl', 'rb') as handle:\n",
    "    cs_train = pickle.load(handle)\n",
    "with open('./train_data/de_train.pkl', 'rb') as handle:\n",
    "    de_train = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c878f50-ba52-4744-abd0-c8017bcfbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_test_dataloader = Custom_Dataloader(cs_test, 64)\n",
    "de_test_dataloader = Custom_Dataloader(de_test, 64)\n",
    "\n",
    "cs_train_dataloader = Custom_Dataloader(cs_train, 96)\n",
    "de_train_dataloader = Custom_Dataloader(de_train, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e9c9b23-aa96-4dca-9c21-6d199a0e0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad is False:\n",
    "        param.requires_grad =  True\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dce6f77-9a84-4719-9fcf-78b107329039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75695 78084\n"
     ]
    }
   ],
   "source": [
    "print(cs_train_dataloader.length, de_train_dataloader.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38640b38-b026-4b38-ab0d-15097f61d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c66fd502-e4a4-4d79-94d3-5e1a72945b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.000\tBatch: 299.000, Loss: 3.432\n",
      " CS Test Loss:  3.3514182567596436 , DE Test Loss:  3.2881734371185303 , Best Averaged Loss:  3.319795846939087\n",
      "Epoch 1.000\tBatch: 299.000, Loss: 1.063\n",
      " CS Test Loss:  2.590630292892456 , DE Test Loss:  2.7482502460479736 , Best Averaged Loss:  2.669440269470215\n",
      "Epoch 2.000\tBatch: 299.000, Loss: 2.356\n",
      " CS Test Loss:  2.270009756088257 , DE Test Loss:  2.421527624130249 , Best Averaged Loss:  2.345768690109253\n",
      "Epoch 3.000\tBatch: 299.000, Loss: 1.404\n",
      " CS Test Loss:  2.099003314971924 , DE Test Loss:  2.2778680324554443 , Best Averaged Loss:  2.188435673713684\n",
      "Epoch 4.000\tBatch: 299.000, Loss: 1.114\n",
      " CS Test Loss:  1.9851433038711548 , DE Test Loss:  2.1842100620269775 , Best Averaged Loss:  2.084676682949066\n",
      "Epoch 5.000\tBatch: 299.000, Loss: 1.224\n",
      " CS Test Loss:  1.9092463254928589 , DE Test Loss:  2.119856119155884 , Best Averaged Loss:  2.0145512223243713\n",
      "Epoch 6.000\tBatch: 299.000, Loss: 1.354\n",
      " CS Test Loss:  1.8840500116348267 , DE Test Loss:  2.0766921043395996 , Best Averaged Loss:  1.9803710579872131\n",
      "Epoch 7.000\tBatch: 299.000, Loss: 2.926\n",
      " CS Test Loss:  1.8198161125183105 , DE Test Loss:  2.04974627494812 , Best Averaged Loss:  1.9347811937332153\n",
      "Epoch 8.000\tBatch: 299.000, Loss: 1.063\n",
      " CS Test Loss:  1.747245192527771 , DE Test Loss:  2.0255351066589355 , Best Averaged Loss:  1.8863901495933533\n",
      "Epoch 9.000\tBatch: 299.000, Loss: 1.202\n",
      " CS Test Loss:  1.6980119943618774 , DE Test Loss:  1.9851487874984741 , Best Averaged Loss:  1.8415803909301758\n",
      "Epoch 10.000\tBatch: 299.000, Loss: 1.961\n",
      " CS Test Loss:  1.674546241760254 , DE Test Loss:  1.951406478881836 , Best Averaged Loss:  1.812976360321045\n",
      "Epoch 11.000\tBatch: 299.000, Loss: 1.602\n",
      " CS Test Loss:  1.6316124200820923 , DE Test Loss:  1.9252262115478516 , Best Averaged Loss:  1.778419315814972\n",
      "Epoch 12.000\tBatch: 299.000, Loss: 2.255\n",
      " CS Test Loss:  1.582565426826477 , DE Test Loss:  1.9100044965744019 , Best Averaged Loss:  1.7462849617004395\n",
      "Epoch 13.000\tBatch: 299.000, Loss: 1.716\n",
      " CS Test Loss:  1.5583189725875854 , DE Test Loss:  1.8985414505004883 , Best Averaged Loss:  1.7284302115440369\n",
      "Epoch 14.000\tBatch: 299.000, Loss: 3.006\n",
      " CS Test Loss:  1.5066322088241577 , DE Test Loss:  1.878085732460022 , Best Averaged Loss:  1.6923589706420898\n",
      "Epoch 15.000\tBatch: 299.000, Loss: 1.165\n",
      " CS Test Loss:  1.5081998109817505 , DE Test Loss:  1.8573306798934937 , Best Averaged Loss:  1.682765245437622\n",
      "Epoch 16.000\tBatch: 299.000, Loss: 1.877\n",
      " CS Test Loss:  1.4745978116989136 , DE Test Loss:  1.855103850364685 , Best Averaged Loss:  1.6648508310317993\n",
      "Epoch 17.000\tBatch: 299.000, Loss: 1.669\n",
      " CS Test Loss:  1.4778965711593628 , DE Test Loss:  1.843562126159668 , Best Averaged Loss:  1.6607293486595154\n",
      "Epoch 18.000\tBatch: 299.000, Loss: 1.287\n",
      " CS Test Loss:  1.4934909343719482 , DE Test Loss:  1.830517053604126 , Best Averaged Loss:  1.662003993988037\n",
      "Epoch 19.000\tBatch: 299.000, Loss: 0.908\n",
      " CS Test Loss:  1.4694321155548096 , DE Test Loss:  1.8277108669281006 , Best Averaged Loss:  1.648571491241455\n",
      "Epoch 20.000\tBatch: 299.000, Loss: 0.879\n",
      " CS Test Loss:  1.4394052028656006 , DE Test Loss:  1.80624520778656 , Best Averaged Loss:  1.6228252053260803\n",
      "Epoch 21.000\tBatch: 299.000, Loss: 0.342\n",
      " CS Test Loss:  1.4380549192428589 , DE Test Loss:  1.7921959161758423 , Best Averaged Loss:  1.6151254177093506\n",
      "Epoch 22.000\tBatch: 299.000, Loss: 1.868\n",
      " CS Test Loss:  1.4157048463821411 , DE Test Loss:  1.7848031520843506 , Best Averaged Loss:  1.6002539992332458\n",
      "Epoch 23.000\tBatch: 299.000, Loss: 0.576\n",
      " CS Test Loss:  1.4015699625015259 , DE Test Loss:  1.7770112752914429 , Best Averaged Loss:  1.5892906188964844\n",
      "Epoch 24.000\tBatch: 299.000, Loss: 1.323\n",
      " CS Test Loss:  1.4217785596847534 , DE Test Loss:  1.76296865940094 , Best Averaged Loss:  1.5923736095428467\n",
      "Epoch 25.000\tBatch: 299.000, Loss: 1.247\n",
      " CS Test Loss:  1.3975223302841187 , DE Test Loss:  1.7616069316864014 , Best Averaged Loss:  1.57956463098526\n",
      "Epoch 26.000\tBatch: 299.000, Loss: 1.722\n",
      " CS Test Loss:  1.3924659490585327 , DE Test Loss:  1.761850118637085 , Best Averaged Loss:  1.5771580338478088\n",
      "Epoch 27.000\tBatch: 299.000, Loss: 1.816\n",
      " CS Test Loss:  1.3851871490478516 , DE Test Loss:  1.7566782236099243 , Best Averaged Loss:  1.570932686328888\n",
      "Epoch 28.000\tBatch: 299.000, Loss: 1.954\n",
      " CS Test Loss:  1.3802303075790405 , DE Test Loss:  1.7485352754592896 , Best Averaged Loss:  1.564382791519165\n",
      "Epoch 29.000\tBatch: 299.000, Loss: 0.927\n",
      " CS Test Loss:  1.352626085281372 , DE Test Loss:  1.7403180599212646 , Best Averaged Loss:  1.5464720726013184\n",
      "Epoch 30.000\tBatch: 299.000, Loss: 1.543\n",
      " CS Test Loss:  1.3648563623428345 , DE Test Loss:  1.7353392839431763 , Best Averaged Loss:  1.5500978231430054\n",
      "Epoch 31.000\tBatch: 299.000, Loss: 2.995\n",
      " CS Test Loss:  1.3556747436523438 , DE Test Loss:  1.7349175214767456 , Best Averaged Loss:  1.5452961325645447\n",
      "Epoch 32.000\tBatch: 299.000, Loss: 1.427\n",
      " CS Test Loss:  1.3474191427230835 , DE Test Loss:  1.726810336112976 , Best Averaged Loss:  1.5371147394180298\n",
      "Epoch 33.000\tBatch: 299.000, Loss: 0.800\n",
      " CS Test Loss:  1.342093825340271 , DE Test Loss:  1.7177621126174927 , Best Averaged Loss:  1.5299279689788818\n",
      "Epoch 34.000\tBatch: 299.000, Loss: 0.798\n",
      " CS Test Loss:  1.337324857711792 , DE Test Loss:  1.711076021194458 , Best Averaged Loss:  1.524200439453125\n",
      "Epoch 35.000\tBatch: 299.000, Loss: 1.476\n",
      " CS Test Loss:  1.3357739448547363 , DE Test Loss:  1.7124903202056885 , Best Averaged Loss:  1.5241321325302124\n",
      "Epoch 36.000\tBatch: 299.000, Loss: 1.925\n",
      " CS Test Loss:  1.340416431427002 , DE Test Loss:  1.7079650163650513 , Best Averaged Loss:  1.5241907238960266\n",
      "Epoch 37.000\tBatch: 299.000, Loss: 0.867\n",
      " CS Test Loss:  1.3331081867218018 , DE Test Loss:  1.6999523639678955 , Best Averaged Loss:  1.5165302753448486\n",
      "Epoch 38.000\tBatch: 299.000, Loss: 1.882\n",
      " CS Test Loss:  1.3320354223251343 , DE Test Loss:  1.7011533975601196 , Best Averaged Loss:  1.516594409942627\n",
      "Epoch 39.000\tBatch: 299.000, Loss: 0.947\n",
      " CS Test Loss:  1.3322219848632812 , DE Test Loss:  1.7006886005401611 , Best Averaged Loss:  1.5164552927017212\n",
      "Epoch 40.000\tBatch: 299.000, Loss: 2.706\n",
      " CS Test Loss:  1.3122336864471436 , DE Test Loss:  1.693338394165039 , Best Averaged Loss:  1.5027860403060913\n",
      "Epoch 41.000\tBatch: 299.000, Loss: 1.660\n",
      " CS Test Loss:  1.3137720823287964 , DE Test Loss:  1.6916718482971191 , Best Averaged Loss:  1.5027219653129578\n",
      "Epoch 42.000\tBatch: 299.000, Loss: 0.516\n",
      " CS Test Loss:  1.3119401931762695 , DE Test Loss:  1.692769169807434 , Best Averaged Loss:  1.5023546814918518\n",
      "Epoch 43.000\tBatch: 299.000, Loss: 0.767\n",
      " CS Test Loss:  1.300562858581543 , DE Test Loss:  1.6795240640640259 , Best Averaged Loss:  1.4900434613227844\n",
      "Epoch 44.000\tBatch: 299.000, Loss: 1.143\n",
      " CS Test Loss:  1.3031370639801025 , DE Test Loss:  1.6863224506378174 , Best Averaged Loss:  1.49472975730896\n",
      "Epoch 45.000\tBatch: 299.000, Loss: 2.127\n",
      " CS Test Loss:  1.2894859313964844 , DE Test Loss:  1.6790533065795898 , Best Averaged Loss:  1.484269618988037\n",
      "Epoch 46.000\tBatch: 299.000, Loss: 1.096\n",
      " CS Test Loss:  1.280832052230835 , DE Test Loss:  1.6720356941223145 , Best Averaged Loss:  1.4764338731765747\n",
      "Epoch 47.000\tBatch: 299.000, Loss: 0.866\n",
      " CS Test Loss:  1.2736419439315796 , DE Test Loss:  1.6740483045578003 , Best Averaged Loss:  1.47384512424469\n",
      "Epoch 48.000\tBatch: 299.000, Loss: 0.919\n",
      " CS Test Loss:  1.2784547805786133 , DE Test Loss:  1.6713697910308838 , Best Averaged Loss:  1.4749122858047485\n",
      "Epoch 49.000\tBatch: 299.000, Loss: 0.902\n",
      " CS Test Loss:  1.274715781211853 , DE Test Loss:  1.6738231182098389 , Best Averaged Loss:  1.474269449710846\n",
      "Epoch 50.000\tBatch: 299.000, Loss: 1.177\n",
      " CS Test Loss:  1.266450047492981 , DE Test Loss:  1.6651719808578491 , Best Averaged Loss:  1.465811014175415\n",
      "Epoch 51.000\tBatch: 299.000, Loss: 1.112\n",
      " CS Test Loss:  1.2770617008209229 , DE Test Loss:  1.668680191040039 , Best Averaged Loss:  1.472870945930481\n",
      "Epoch 52.000\tBatch: 299.000, Loss: 0.765\n",
      " CS Test Loss:  1.2679396867752075 , DE Test Loss:  1.661828637123108 , Best Averaged Loss:  1.4648841619491577\n",
      "Epoch 53.000\tBatch: 299.000, Loss: 1.470\n",
      " CS Test Loss:  1.2675988674163818 , DE Test Loss:  1.6596912145614624 , Best Averaged Loss:  1.4636450409889221\n",
      "Epoch 54.000\tBatch: 299.000, Loss: 1.296\n",
      " CS Test Loss:  1.2689114809036255 , DE Test Loss:  1.6585867404937744 , Best Averaged Loss:  1.4637491106987\n",
      "Epoch 55.000\tBatch: 299.000, Loss: 1.105\n",
      " CS Test Loss:  1.2635130882263184 , DE Test Loss:  1.6584537029266357 , Best Averaged Loss:  1.460983395576477\n",
      "Epoch 56.000\tBatch: 299.000, Loss: 1.003\n",
      " CS Test Loss:  1.2626841068267822 , DE Test Loss:  1.6555427312850952 , Best Averaged Loss:  1.4591134190559387\n",
      "Epoch 57.000\tBatch: 299.000, Loss: 1.496\n",
      " CS Test Loss:  1.2623385190963745 , DE Test Loss:  1.652728796005249 , Best Averaged Loss:  1.4575336575508118\n",
      "Epoch 58.000\tBatch: 299.000, Loss: 1.437\n",
      " CS Test Loss:  1.255207896232605 , DE Test Loss:  1.6529124975204468 , Best Averaged Loss:  1.4540601968765259\n",
      "Epoch 59.000\tBatch: 299.000, Loss: 1.039\n",
      " CS Test Loss:  1.2613738775253296 , DE Test Loss:  1.6525286436080933 , Best Averaged Loss:  1.4569512605667114\n",
      "Epoch 60.000\tBatch: 299.000, Loss: 1.020\n",
      " CS Test Loss:  1.2544732093811035 , DE Test Loss:  1.6514531373977661 , Best Averaged Loss:  1.4529631733894348\n",
      "Epoch 61.000\tBatch: 299.000, Loss: 1.060\n",
      " CS Test Loss:  1.2575336694717407 , DE Test Loss:  1.648997187614441 , Best Averaged Loss:  1.4532654285430908\n",
      "Epoch 62.000\tBatch: 299.000, Loss: 1.768\n",
      " CS Test Loss:  1.2480711936950684 , DE Test Loss:  1.6456187963485718 , Best Averaged Loss:  1.44684499502182\n",
      "Epoch 63.000\tBatch: 299.000, Loss: 0.470\n",
      " CS Test Loss:  1.2492482662200928 , DE Test Loss:  1.645518183708191 , Best Averaged Loss:  1.4473832249641418\n",
      "Epoch 64.000\tBatch: 299.000, Loss: 1.646\n",
      " CS Test Loss:  1.252126693725586 , DE Test Loss:  1.6439270973205566 , Best Averaged Loss:  1.4480268955230713\n",
      "Epoch 65.000\tBatch: 15.000, Loss: 0.228"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33812/2840122666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m saved_model_state = train_loop(model, \n\u001b[0m\u001b[0;32m      2\u001b[0m                               \u001b[0mcs_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                               \u001b[0mde_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[0mcs_test_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[0mde_test_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33812/3811059228.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, lang1_train_dataloader, lang2_train_dataloader, lang1_test_dataloader, lang2_test_dataloader, epochs, steps)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         train(model, \n\u001b[0m\u001b[0;32m     23\u001b[0m               \u001b[0mlang1_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m               \u001b[0mlang2_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33812/3811059228.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, lang1_train_dataloader, lang2_train_dataloader, optim, steps, epoch)\u001b[0m\n\u001b[0;32m     62\u001b[0m         print('\\rEpoch {:.3f}\\tBatch: {:.3f}, Loss: {:.3f}'.format(epoch, \n\u001b[0;32m     63\u001b[0m                                                                    \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                                                                    loss.detach().item()), \n\u001b[0m\u001b[0;32m     65\u001b[0m               end=\"\")\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saved_model_state = train_loop(model, \n",
    "                              cs_train_dataloader,\n",
    "                              de_train_dataloader,\n",
    "                              cs_test_dataloader,\n",
    "                              de_test_dataloader,\n",
    "                              65,\n",
    "                               './best_model.pth',\n",
    "                             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
