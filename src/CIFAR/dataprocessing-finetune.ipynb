{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442e8cd1-f131-4be3-aca1-21931451c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import segmentation_models_pytorch as smp\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4fe56-0a0a-4a32-9bfb-bcaa4c43b9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dca2815-d398-4ebb-9fce-2d1b4ca03499",
   "metadata": {},
   "source": [
    "## Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62030012-fb37-4810-a1e4-dde9025d6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc337b46-acda-44c9-8356-2440295a4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = {}\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle('./cifar/data_batch_'+str(i))\n",
    "    labels = batch[b'labels']\n",
    "    data = batch[b'data']\n",
    "    for j, label in enumerate(labels):\n",
    "        if label not in all_train_data:\n",
    "            all_train_data.update({label:[torch.tensor(data[j]).reshape((3, 32, 32))]})\n",
    "        else:\n",
    "            curr = all_train_data[label] \n",
    "            curr.append(torch.tensor(data[j]).reshape((3, 32, 32)))\n",
    "            all_train_data.update({label:curr})\n",
    "            \n",
    "all_test_data = {}\n",
    "\n",
    "batch = unpickle('./cifar/test_batch')\n",
    "labels = batch[b'labels']\n",
    "data = batch[b'data']\n",
    "for j, label in enumerate(labels):\n",
    "    if label not in all_test_data:\n",
    "        all_test_data.update({label:[torch.tensor(data[j]).reshape((3, 32, 32))]})\n",
    "    else:\n",
    "        curr = all_test_data[label] \n",
    "        curr.append(torch.tensor(data[j]).reshape((3, 32, 32)))\n",
    "        all_test_data.update({label:curr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32348bd4-6e87-4c34-9590-4cf6eb902886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028c188-3ab7-4d43-af2f-79708068a71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d3f08449a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(  all_test_data[4][6].permute([1, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012d0f4-56ec-49e4-bb98-4c454362bd13",
   "metadata": {},
   "source": [
    "# Big tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4789441d-5cdc-4955-b508-cb389cbcbcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicles\n",
      "animals\n",
      "vehicles\n",
      "animals\n"
     ]
    }
   ],
   "source": [
    "vehicles = [0, 1, 8, 9]\n",
    "animals = [3, 4, 5, 7]\n",
    "\n",
    "tasks = [vehicles, animals]\n",
    "task_names = ['vehicles', 'animals']\n",
    "\n",
    "for task in all_train_data:\n",
    "    if not os.path.exists('./cifar/train/' + str(task)):\n",
    "        os.mkdir('./cifar/train/' + str(task)) \n",
    "    for image_idx in range(len(all_train_data[task])):\n",
    "        image = all_train_data[task][image_idx]\n",
    "        save_image(all_train_data[task][image_idx]/255., './cifar/train/'+ str(task) + '/' + str(image_idx) + '.png')\n",
    "        \n",
    "for task in all_test_data:\n",
    "    if not os.path.exists('./cifar/test/' + str(task)):\n",
    "        os.mkdir('./cifar/test/' + str(task)) \n",
    "    for image_idx in range(len(all_test_data[task])):\n",
    "        image = all_test_data[task][image_idx]\n",
    "        save_image(all_test_data[task][image_idx]/255., './cifar/test/'+ str(task) + '/' + str(image_idx) + '.png')\n",
    "        \n",
    "for i, task in enumerate(task_names):\n",
    "    \n",
    "    if not os.path.exists('./cifar/ext/' + str(task)):\n",
    "        os.mkdir('./cifar/ext/' + str(task)) \n",
    "    print(task)\n",
    "\n",
    "    for k in tasks[i]:  \n",
    "\n",
    "        counter = 0\n",
    "        lim = 5000\n",
    "        if not os.path.exists('./cifar/ext/' + str(task)+ '/' + str(k)):\n",
    "            os.mkdir('./cifar/ext/' + str(task)+ '/' + str(k)) \n",
    "        for image_idx in range(len(all_train_data[k])):\n",
    "            image = all_train_data[k][image_idx]\n",
    "            save_image(all_train_data[k][image_idx]/255., './cifar/ext/' + str(task) + '/' + str(k) + '/' + str(image_idx) + '.png')\n",
    "            counter += 1 \n",
    "            if counter == lim:\n",
    "                break \n",
    "                \n",
    "for i, task in enumerate(task_names):\n",
    "    \n",
    "    if not os.path.exists('./cifar/eval/' + str(task)):\n",
    "        os.mkdir('./cifar/eval/' + str(task)) \n",
    "    print(task)\n",
    "\n",
    "\n",
    "    for k in tasks[i]:  \n",
    "\n",
    "        counter = 0\n",
    "        lim = 1000\n",
    "        if not os.path.exists('./cifar/eval/' + str(task)+ '/' + str(k)):\n",
    "            os.mkdir('./cifar/eval/' + str(task)+ '/' + str(k)) \n",
    "        for image_idx in range(len(all_train_data[k])):\n",
    "            image = all_train_data[k][image_idx]\n",
    "            save_image(all_train_data[k][image_idx]/255., './cifar/eval/' + str(task) + '/' + str(k) + '/' + str(image_idx) + '.png')\n",
    "            counter += 1 \n",
    "            if counter == lim:\n",
    "                break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4bfa8-6e14-4270-b42a-a11600ab2581",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Binary Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d062a56-8836-4b9d-93fb-4a30f26763e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_plane = [0, 2]\n",
    "cat_dog = [3, 5]\n",
    "cerv_equine = [4, 7]\n",
    "car_truck = [1, 9]\n",
    "\n",
    "tasks = [bird_plane, cat_dog, cerv_equine, car_truck]\n",
    "task_names = ['bird_plane', 'cat_dog', 'cerv_equine', 'car_truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f94b4e-991b-4405-8928-17a657afebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in all_train_data:\n",
    "    if not os.path.exists('./cifar/train/' + str(task)):\n",
    "        os.mkdir('./cifar/train/' + str(task)) \n",
    "    for image_idx in range(len(all_train_data[task])):\n",
    "        image = all_train_data[task][image_idx]\n",
    "        save_image(all_train_data[task][image_idx]/255., './cifar/train/'+ str(task) + '/' + str(image_idx) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd59e20-9341-4ba2-879e-c5458853e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in all_test_data:\n",
    "    if not os.path.exists('./cifar/test/' + str(task)):\n",
    "        os.mkdir('./cifar/test/' + str(task)) \n",
    "    for image_idx in range(len(all_test_data[task])):\n",
    "        image = all_test_data[task][image_idx]\n",
    "        save_image(all_test_data[task][image_idx]/255., './cifar/test/'+ str(task) + '/' + str(image_idx) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be713c-1dfb-4656-9f6f-d991fa12f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird_plane\n",
      "cat_dog\n",
      "cerv_equine\n",
      "car_truck\n"
     ]
    }
   ],
   "source": [
    "for i, task in enumerate(task_names):\n",
    "    \n",
    "    if not os.path.exists('./cifar/ext/' + str(task)):\n",
    "        os.mkdir('./cifar/ext/' + str(task)) \n",
    "    print(task)\n",
    "\n",
    "    for k in tasks[i]:  \n",
    "\n",
    "        counter = 0\n",
    "        lim = 2000\n",
    "        if not os.path.exists('./cifar/ext/' + str(task)+ '/' + str(k)):\n",
    "            os.mkdir('./cifar/ext/' + str(task)+ '/' + str(k)) \n",
    "        for image_idx in range(len(all_train_data[k])):\n",
    "            image = all_train_data[k][image_idx]\n",
    "            save_image(all_train_data[k][image_idx]/255., './cifar/ext/' + str(task) + '/' + str(k) + '/' + str(image_idx) + '.png')\n",
    "            counter += 1 \n",
    "            if counter == lim:\n",
    "                break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa4e1d-4f48-4f7c-a8cb-64b58a9cfa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird_plane\n",
      "cat_dog\n",
      "cerv_equine\n",
      "car_truck\n"
     ]
    }
   ],
   "source": [
    "for i, task in enumerate(task_names):\n",
    "    \n",
    "    if not os.path.exists('./cifar/eval/' + str(task)):\n",
    "        os.mkdir('./cifar/eval/' + str(task)) \n",
    "    print(task)\n",
    "\n",
    "\n",
    "    for k in tasks[i]:  \n",
    "\n",
    "        counter = 0\n",
    "        lim = 1000\n",
    "        if not os.path.exists('./cifar/eval/' + str(task)+ '/' + str(k)):\n",
    "            os.mkdir('./cifar/eval/' + str(task)+ '/' + str(k)) \n",
    "        for image_idx in range(len(all_train_data[k])):\n",
    "            image = all_train_data[k][image_idx]\n",
    "            save_image(all_train_data[k][image_idx]/255., './cifar/eval/' + str(task) + '/' + str(k) + '/' + str(image_idx) + '.png')\n",
    "            counter += 1 \n",
    "            if counter == lim:\n",
    "                break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb7bae-9faf-41ed-bd4c-1f79700f0f1c",
   "metadata": {},
   "source": [
    "# Non Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0736b0-c589-415a-a797-6725aefaae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "                        \n",
    "#                         nn.Conv2d(3,512,2),\n",
    "#                         nn.Dropout2d(0.2),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.2),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.MaxPool2d(2),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.4),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.4),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.MaxPool2d(2),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.4),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.4),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.MaxPool2d(2),\n",
    "#                         nn.Conv2d(512,512,2),\n",
    "#                         nn.Dropout2d(0.4),\n",
    "#                         nn.BatchNorm2d(512),\n",
    "#                         nn.SiLU(),\n",
    "#                         nn.Flatten()\n",
    "#                             ) \n",
    "#         self.fc = nn.Sequential(\n",
    "#                                 nn.Linear(512, num_classes)\n",
    "#                                 ) \n",
    "#         self.model = nn.Sequential(self.conv, self.fc)\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.weights_backup = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def apply_mask(self, mask, sizing):\n",
    "        start = 0\n",
    "        copy_state = copy.deepcopy(self.model.state_dict())\n",
    "        segments = {}\n",
    "        for i in copy_state:\n",
    "            if i in sizing:\n",
    "                end = start + sizing[i]\n",
    "                segment = np.round(mask[start:end])\n",
    "                index = np.where(segment == 0)\n",
    "            \n",
    "                copy_state[i].data[index] = 0\n",
    "                segments.update({i:index})\n",
    "                    \n",
    "                start = end\n",
    "                # print(i)\n",
    "        self.model.load_state_dict(copy_state)\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in segments:\n",
    "                param.data[segments[name]].requires_grad = False\n",
    "                start = end\n",
    "        \n",
    "        \n",
    "        \n",
    "    def return_model(self):\n",
    "        return self.model \n",
    "    \n",
    "    def return_model_state(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def revert_weights(self):\n",
    "        self.model.load_state_dict(self.weights_backup)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def update_backup(self):\n",
    "        self.weights_backup = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "def size_mask(state_dict):\n",
    "    total = 0\n",
    "    mask_sizing = OrderedDict()\n",
    "    for i in list(state_dict.keys()):  \n",
    "        # print(i)\n",
    "        # print(state_dict[i].shape)\n",
    "        if 'conv' in i and 'weight' in i:\n",
    "            shape = state_dict[i].shape\n",
    "            if len(shape) > 1:\n",
    "                \n",
    "                size1 = shape[0]\n",
    "                total += size1\n",
    "                mask_sizing.update({i:size1})\n",
    "    print(total)\n",
    "    return mask_sizing\n",
    "\n",
    "def dropout_mask(dim, lim1, lim2):\n",
    "    np.random.seed()\n",
    "    sample = np.random.uniform(lim1, lim2, 1)[0]\n",
    "    num_indices = np.int_(sample*dim)\n",
    "    indices = np.random.choice(range(0, dim), num_indices, replace=False)\n",
    "    base = np.random.uniform(0, 0.5, dim)\n",
    "    base[indices] += 0.5\n",
    "\n",
    "    return base\n",
    "\n",
    "def collapse_prevention(mask, sizing):\n",
    "    start = 0\n",
    "    for i in sizing:\n",
    "        end = start + sizing[i]\n",
    "        if sum(np.round(mask[start:end])) == 0:\n",
    "            choice = np.random.choice(range(start, end), 1)[0]\n",
    "            mask[choice] = 1\n",
    "        start = end\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb1c5d-78e1-426b-bcc0-f3bba8707eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904\n",
      "2611.0\n"
     ]
    }
   ],
   "source": [
    "test = CNN(1)\n",
    "mask_sizing = size_mask(test.return_model_state())\n",
    "mask = dropout_mask(3904, 0.3, 0.99)\n",
    "mask = collapse_prevention(mask, mask_sizing)\n",
    "test.revert_weights()\n",
    "\n",
    "print(np.sum(np.round(mask)))\n",
    "    \n",
    "test.apply_mask(mask, mask_sizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0120b1-4241-466d-b072-c95fdd3ef289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904\n",
      "[[64, 3, 3, 3], [64, 64, 3, 3], [64, 64, 3, 3], [64, 64, 3, 3], [64, 64, 3, 3], [128, 64, 3, 3], [128, 128, 3, 3], [128, 128, 3, 3], [128, 128, 3, 3], [256, 128, 3, 3], [256, 256, 3, 3], [256, 256, 3, 3], [256, 256, 3, 3], [512, 256, 3, 3], [512, 512, 3, 3], [512, 512, 3, 3], [512, 512, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "model = CNN(10)\n",
    "\n",
    "# summary(a, (1, 3, 32, 32))\n",
    "\n",
    "mask_sizing = size_mask(model.return_model_state())\n",
    "out = {}\n",
    "all_shapes = []\n",
    "state = model.return_model_state()\n",
    "for i in mask_sizing:\n",
    "    all_shapes.append(list(state[i].shape))\n",
    "    out.update({i:state[i].shape})\n",
    "    \n",
    "print(all_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f6b3c-6ad0-44fb-8c73-a371e1ffeaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9309e2-d38f-4a88-9eed-98f7fa3d7398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[list(model.state_dict().keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b6f7d-c5fb-4f41-885e-e03c195b53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f649e-299d-4a89-84bc-8a34a04b6147",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef6db235-db82-40c3-adc6-e35e3b6f8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE_1 = torch.device('cuda:0')\n",
    "# DEVICE_2 = torch.device('cpu')\n",
    "    \n",
    "\n",
    "\n",
    "# class masked_model(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.model = models.efficientnet_b3(pretrained=True)\n",
    "#         self.model.features[0] = nn.Sequential(\n",
    "#                                               nn.Conv2d(3, 40, 3, 1, 1, bias=False),\n",
    "#                                               nn.BatchNorm2d(40),\n",
    "#                                               nn.SiLU(inplace=True)\n",
    "#                                               )\n",
    "#         self.model.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "#                                             nn.Linear(1536, num_classes),\n",
    "#                                             )\n",
    "        \n",
    "#         for m in self.model.modules():\n",
    "#             if isinstance(m, nn.Dropout):\n",
    "#                 m.p = 0.5\n",
    "                \n",
    "#         self.weights_backup = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def apply_mask(self, mask, sizing):\n",
    "#         start = 0\n",
    "#         copy_state = copy.deepcopy(self.model.state_dict())\n",
    "#         for i in copy_state:\n",
    "#             if i in sizing:\n",
    "#                 end = start + sizing[i][0]\n",
    "#                 segment = np.round(mask[start:end])\n",
    "#                 index = np.where(segment == 0)\n",
    "#                 shape = copy_state[i].shape\n",
    "#                 size1 = shape[0]\n",
    "#                 size2 = shape[1]\n",
    "\n",
    "#                 if size1 > size2:\n",
    "#                     copy_state[i].data[index] = 0\n",
    "#                 else:\n",
    "#                     copy_state[i].data[:, index] = 0\n",
    "                    \n",
    "#                 start = end\n",
    "#                 # print(i)\n",
    "#         self.model.load_state_dict(copy_state)\n",
    "        \n",
    "#     def return_model(self):\n",
    "#         return self.model \n",
    "    \n",
    "#     def return_model_state(self):\n",
    "#         return self.model.state_dict()\n",
    "\n",
    "#     def revert_weights(self):\n",
    "#         return self.model.load_state_dict(self.weights_backup)\n",
    "    \n",
    "#     def update_backup(self):\n",
    "#         self.weights_backup = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "#     def update_trained_weights(self, mask, sizing, device):\n",
    "#         current_weights = copy.deepcopy(self.model.state_dict())\n",
    "#         start = 0\n",
    "#         for i in self.model.state_dict():\n",
    "#             if i in sizing:\n",
    "#                 end = start + sizing[i][0]\n",
    "#                 segment = np.round(mask[start:end])\n",
    "#                 index = np.where(segment == 0)\n",
    "#                 shape = current_weights[i].shape\n",
    "#                 if len(shape) > 1:\n",
    "#                     size1 = shape[0]\n",
    "#                     size2 = shape[1]\n",
    "\n",
    "#                 if size1 > size2:\n",
    "#                     current_weights[i].data[index] = self.weights_backup[i].data[index]\n",
    "#                 else:\n",
    "#                     current_weights[i].data[:, index] = self.weights_backup[i].data[:, index]\n",
    "                    \n",
    "#                 # current_weights[i].data[index] = self.weights_backup[i].data[index]\n",
    "#                 start = end\n",
    "                \n",
    "#         self.model.load_state_dict(current_weights)\n",
    "\n",
    "# def size_mask(state_dict):\n",
    "#     total = 0\n",
    "#     mask_sizing = OrderedDict()\n",
    "#     for i in list(state_dict.keys()):  \n",
    "#         if 'features' in i and 'weight' in i and 'fc' not in i:\n",
    "#             shape = state_dict[i].shape\n",
    "#             if len(shape) > 1:\n",
    "#                 size1 = shape[0]\n",
    "#                 size2 = shape[1]\n",
    "\n",
    "#                 if size1 > size2:\n",
    "#                     if size1>816:\n",
    "#                         print(shape)\n",
    "#                         total += size1\n",
    "#                         mask_sizing.update({i:(size1, shape)})\n",
    "#                 else:\n",
    "#                     if size2>816:\n",
    "#                         print(shape)\n",
    "#                         total += size2\n",
    "#                         mask_sizing.update({i:(size2, shape)})\n",
    "\n",
    "#     print(total)\n",
    "#     return mask_sizing\n",
    "\n",
    "# def dropout_mask(dim, lim1, lim2):\n",
    "#     np.random.seed()\n",
    "#     sample = np.random.uniform(lim1, lim2, 1)[0]\n",
    "#     num_indices = np.int_(sample*dim)\n",
    "#     indices = np.random.choice(range(0, dim), num_indices, replace=False)\n",
    "#     base = np.random.uniform(0, 0.5, dim)\n",
    "#     base[indices] += 0.5\n",
    "\n",
    "#     return base\n",
    "\n",
    "# def collapse_prevention(mask, sizing):\n",
    "#     start = 0\n",
    "#     for i in sizing:\n",
    "#         end = start + sizing[i][0]\n",
    "#         if sum(np.round(mask[start:end])) == 0:\n",
    "#             choice = np.random.choice(range(start, end), 1)[0]\n",
    "#             mask[choice] = 1\n",
    "#         start = end\n",
    "#     return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908c452-56b1-477f-8984-a1bcd6879a0f",
   "metadata": {},
   "source": [
    "# Curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588d2b0-7ad4-4eb5-9431-981a2bc0e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_1 = torch.device('cuda:0')\n",
    "DEVICE_2 =  torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc24d72-364d-46fd-9ed2-517a24c132e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304c3ee-4b37-47f8-9ccf-edcc38c52a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817b3a1-c65c-4ee9-bd4c-52f8d16b0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './cifar/train/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                # transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.5),\n",
    "                                # transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "                                # transforms.Resize((224, 224)),\n",
    "                                transforms.GaussianBlur(1, sigma=(0.2, 0.5)),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "trainloader = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066421d9-0ce3-4b2a-bb3e-766e35eca83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './cifar/test/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset = datasets.ImageFolder(test_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "testloader = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f4d43-26d6-4d04-8902-31b4c7d0ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(DEVICE_1)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "lmbda = lambda epoch: 0.9\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optim, lr_lambda=lmbda)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821eedd0-cc9c-41ba-ab1f-6c1b6d614b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE_1)\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    # model.update_backup()\n",
    "    counter = 0\n",
    "    # mask = dropout_mask(3904, 0.9, 0.99)\n",
    "    # mask = collapse_prevention(mask, mask_sizing)\n",
    "    # model.apply_mask(mask, mask_sizing)\n",
    "            \n",
    "    for i, batch in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch[0].to(DEVICE_1), batch[1].to(DEVICE_1)\n",
    "            fx = model(x)\n",
    "            loss = criterion(fx.squeeze(), y)\n",
    "            avg_loss += loss.detach().item()\n",
    "            \n",
    "            _, predicted = fx.max(1)\n",
    "            \n",
    "            acc_per_batch = 100. * predicted.eq(y).sum().item() / y.size(0)\n",
    "            avg_acc += acc_per_batch\n",
    "            counter += 1\n",
    "            x = x.to(DEVICE_2)\n",
    "            y = y.to(DEVICE_2)\n",
    "    # model.revert_weights()\n",
    "    avg_acc /= counter\n",
    "    model.train()\n",
    "    print('\\nAverage Accuracy: {:.3f}'.format(avg_acc, end=\"\"))\n",
    "    print('\\nAverage Validation Loss: {:.3f}'.format(avg_loss/len(testloader), end=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec062602-6048-4194-ae3c-b253e0ebc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34660/1813836376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sim\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34660/322691105.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sim\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34660/2320189138.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_quantized\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_pruned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprune_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_amount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_quantized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_tensor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_pruned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantize_bit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;31m#after we do inference, we get the ofmap size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_quantized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "state = model.return_model_state()\n",
    "mask_sizing = size_mask(state)\n",
    "model = model.cuda()\n",
    "model.train()\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        loss_epoch = 0\n",
    "        acc_epoch = 0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            # mask = dropout_mask(33504, 0.5, 0.99)\n",
    "            # mask = collapse_prevention(mask, mask_sizing)\n",
    "            # model.update_backup()\n",
    "            # model.apply_mask(mask, mask_sizing)\n",
    "            \n",
    "            # model = model.cuda()\n",
    "            \n",
    "            # print(np.sum(mask))\n",
    "            # print(np.sum(np.round(mask)))\n",
    "            \n",
    "\n",
    "            x, y = batch[0].cuda(), batch[1].cuda()\n",
    "\n",
    "            fx = model(x)\n",
    "            \n",
    "            _, predicted = fx.max(1)\n",
    "            \n",
    "            acc_per_batch = 100. * predicted.eq(y).sum().item() / y.size(0)\n",
    "            acc_epoch += acc_per_batch\n",
    "            \n",
    "            loss = criterion(fx.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            print('\\rEpoch {}\\tBatch: {:.3f}, Loss: {:.3f}, Acc:{:.3f}'.format(epoch, i, loss.detach().item(), acc_per_batch), end=\"\")\n",
    "            x = x.cpu()\n",
    "            y = y.cpu()\n",
    "            loss_epoch += loss.detach().item()\n",
    "            # model = model.cpu()\n",
    "            # model.update_trained_weights(mask, mask_sizing, DEVICE_1)\n",
    "            # model.revert_weights()\n",
    "        test(model, testloader)\n",
    "        scheduler.step()\n",
    "        print('Average Loss: {:.3f}'.format(loss_epoch/len(trainloader), end=\"\"))\n",
    "        torch.save(model.return_model_state(), './cifarmodels/' + str(epoch) + '.pth')\n",
    "\n",
    "    model = model.cpu()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    model = model.cpu()\n",
    "    x = x.cpu()\n",
    "    y = y.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7c4e9-1d49-4d6a-bf7d-17885c0bfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "e1c2bb42-f503-43ed-bd2b-5a7c81baae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Accuracy: 97.833\n",
      "\n",
      "Average Validation Loss: 0.100\n",
      "\n",
      "Average Accuracy: 90.833\n",
      "\n",
      "Average Validation Loss: 0.268\n",
      "\n",
      "Average Accuracy: 98.000\n",
      "\n",
      "Average Validation Loss: 0.113\n",
      "\n",
      "Average Accuracy: 97.500\n",
      "\n",
      "Average Validation Loss: 0.101\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE_1)\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    model.update_backup()\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    # mask = dropout_mask(3904, 0.5, 0.99)\n",
    "    # mask = collapse_prevention(mask, mask_sizing)\n",
    "    # model.apply_mask(mask, mask_sizing)\n",
    "    # print(np.sum(np.round(mask)))\n",
    "    for i, batch in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch[0].to(DEVICE_1), batch[1].to(DEVICE_1)\n",
    "            fx = model(x)\n",
    "            loss = criterion(fx.squeeze(), y)\n",
    "            avg_loss += loss.detach().item()\n",
    "            \n",
    "            _, predicted = fx.max(1)\n",
    "            \n",
    "            acc_per_batch = 100. * predicted.eq(y).sum().item() / y.size(0)\n",
    "            avg_acc += acc_per_batch\n",
    "            counter += 1\n",
    "            x = x.to(DEVICE_2)\n",
    "            y = y.to(DEVICE_2)\n",
    "    model.revert_weights()\n",
    "    avg_acc /= counter\n",
    "    model.train()\n",
    "    print('\\nAverage Accuracy: {:.3f}'.format(avg_acc, end=\"\"))\n",
    "    print('\\nAverage Validation Loss: {:.3f}'.format(avg_loss/len(testloader), end=\"\"))\n",
    "    \n",
    "\n",
    "for task_idx, task_name in enumerate(task_names):\n",
    "    test_model = CNN(2)\n",
    "    \n",
    "    state = torch.load('./cifarmodels/4.pth')\n",
    "    test_model.update_backup()\n",
    "    base_state = copy.deepcopy(test_model.state_dict())\n",
    "    idx_arr = tasks[task_idx]\n",
    "    for key in state:\n",
    "        mod_key = 'model.' + key\n",
    "        if 'fc.' not in key:\n",
    "            base_state[mod_key] = state[key]\n",
    "        elif 'fc.' in key and 'weight' in key:\n",
    "            base_params = base_state[mod_key]\n",
    "            # print(state)\n",
    "            for i, idx in enumerate(idx_arr):\n",
    "                base_params[i] = state[key][idx]\n",
    "            base_state[mod_key] = base_params\n",
    "    test_model.load_state_dict(base_state)\n",
    "    eval_path = './cifar/eval/'+task_name+'/'\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    dataset = datasets.ImageFolder(eval_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=600, shuffle=False)\n",
    "    evalloader = dataloader\n",
    "    test(test_model, evalloader)\n",
    "    \n",
    "    torch.save(test_model.state_dict(), './cifarmodels/' + task_name + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "40136f88-6147-4b65-9f6a-308cb2fa7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = CNN(2)\n",
    "test_model.load_state_dict(torch.load('./cifarmodels/bird_plane.pth'))\n",
    "test_model.update_backup()\n",
    "# test(test_model, evalloader)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "efb6814a-fd9b-408b-9bf7-fff384f63dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(test_model, evalloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "5da2627b-9b17-473f-b182-d421d8838d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, evalloader):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(evalloader):\n",
    "        with torch.no_grad():\n",
    "            x, y = batch[0].to(DEVICE_1), batch[1].to(DEVICE_1)\n",
    "            fx = model(x)\n",
    "            loss = criterion(fx.squeeze(), y)\n",
    "            avg_loss += loss.detach().item()\n",
    "            \n",
    "            _, predicted = fx.max(1)\n",
    "            \n",
    "            acc_per_batch = 100. * predicted.eq(y).sum().item() / y.size(0)\n",
    "            avg_acc += acc_per_batch\n",
    "            counter += 1\n",
    "            x = x.to(DEVICE_2)\n",
    "            y = y.to(DEVICE_2)\n",
    "    avg_acc /= counter\n",
    "    print('\\nAverage Accuracy: {:.3f}'.format(avg_acc, end=\"\"))\n",
    "    print('\\nAverage Validation Loss: {:.3f}'.format(avg_loss/len(testloader), end=\"\"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "d7e09a13-75f1-4d06-9468-7b4a5cf9d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetune(model, extloader, evalloader,):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(DEVICE_1)\n",
    "    optim = torch.optim.Adadelta(test_model.parameters())\n",
    "\n",
    "    model = model.to(DEVICE_1)\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    counter = 0\n",
    "    print(\"JAT\")\n",
    "    model.eval()\n",
    "    test(model, evalloader)\n",
    "    mask = dropout_mask(3904, 0., 0.99)\n",
    "    mask = collapse_prevention(mask, mask_sizing)\n",
    "    # print(\"dicker\")\n",
    "    model.apply_mask(mask, mask_sizing)\n",
    "    print(np.sum(np.round(mask)))\n",
    "    print(\"MOT\")\n",
    "    test(model, evalloader)\n",
    "    epochs = 20\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        loss_epoch = 0\n",
    "        acc_epoch = 0\n",
    "        for i, batch in enumerate(extloader):\n",
    "            optim.zero_grad()\n",
    "            x, y = batch[0].cuda(), batch[1].cuda()\n",
    "\n",
    "            fx = model(x)\n",
    "            \n",
    "            _, predicted = fx.max(1)\n",
    "            \n",
    "            acc_per_batch = 100. * predicted.eq(y).sum().item() / y.size(0)\n",
    "            \n",
    "            loss = criterion(fx.squeeze(), y)\n",
    "            \n",
    "            # print(optim)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            print('\\rEpoch {}\\tBatch: {:.3f}, Loss: {:.3f}, Acc:{:.3f}'.format(epoch, i, loss.detach().item(), acc_per_batch), end=\"\")\n",
    "            x = x.cpu()\n",
    "            y = y.cpu()\n",
    "            loss_epoch += loss.detach().item()\n",
    "        \n",
    "        test(model, evalloader)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "2698ea70-241c-4d52-8b76-bcf3e44a38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = './cifar/eval/'+'bird_plane'+'/'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "dataset = datasets.ImageFolder(eval_path, transform=transform)\n",
    "evalloader = DataLoader(dataset, batch_size=600, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "f61b69f6-04f2-4cd0-9241-9c74842ed5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_path = './cifar/ext/'+'bird_plane'+'/'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                # transforms.GaussianBlur(1, sigma=(0.2, 0.5)),\n",
    "                                # transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "dataset = datasets.ImageFolder(ext_path, transform=transform)\n",
    "extloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "161899b9-3984-45a4-a452-2048efb73987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAT\n",
      "\n",
      "Average Accuracy: 97.833\n",
      "\n",
      "Average Validation Loss: 0.002\n",
      "2654.0\n",
      "MOT\n",
      "\n",
      "Average Accuracy: 59.500\n",
      "\n",
      "Average Validation Loss: 0.031\n",
      "Epoch 0\tBatch: 1.000, Loss: 2.135, Acc:77.459\n",
      "Average Accuracy: 50.000\n",
      "\n",
      "Average Validation Loss: 0.159\n",
      "Epoch 1\tBatch: 1.000, Loss: 3.445, Acc:46.721\n",
      "Average Accuracy: 57.833\n",
      "\n",
      "Average Validation Loss: 0.038\n",
      "Epoch 2\tBatch: 1.000, Loss: 0.600, Acc:85.041\n",
      "Average Accuracy: 79.667\n",
      "\n",
      "Average Validation Loss: 0.046\n",
      "Epoch 3\tBatch: 1.000, Loss: 0.592, Acc:70.697\n",
      "Average Accuracy: 86.167\n",
      "\n",
      "Average Validation Loss: 0.010\n",
      "Epoch 4\tBatch: 1.000, Loss: 0.855, Acc:75.205\n",
      "Average Accuracy: 77.333\n",
      "\n",
      "Average Validation Loss: 0.012\n",
      "Epoch 5\tBatch: 1.000, Loss: 0.541, Acc:84.836\n",
      "Average Accuracy: 89.167\n",
      "\n",
      "Average Validation Loss: 0.007\n",
      "Epoch 6\tBatch: 1.000, Loss: 0.264, Acc:90.164\n",
      "Average Accuracy: 91.000\n",
      "\n",
      "Average Validation Loss: 0.006\n",
      "Epoch 7\tBatch: 1.000, Loss: 0.169, Acc:93.648\n",
      "Average Accuracy: 91.667\n",
      "\n",
      "Average Validation Loss: 0.006\n",
      "Epoch 8\tBatch: 1.000, Loss: 0.150, Acc:94.467\n",
      "Average Accuracy: 93.333\n",
      "\n",
      "Average Validation Loss: 0.005\n",
      "Epoch 9\tBatch: 1.000, Loss: 0.157, Acc:92.008\n",
      "Average Accuracy: 89.667\n",
      "\n",
      "Average Validation Loss: 0.006\n",
      "Epoch 10\tBatch: 1.000, Loss: 0.176, Acc:93.033\n",
      "Average Accuracy: 93.833\n",
      "\n",
      "Average Validation Loss: 0.004\n",
      "Epoch 11\tBatch: 1.000, Loss: 0.059, Acc:97.541\n",
      "Average Accuracy: 95.833\n",
      "\n",
      "Average Validation Loss: 0.003\n",
      "Epoch 12\tBatch: 1.000, Loss: 0.031, Acc:98.975\n",
      "Average Accuracy: 96.667\n",
      "\n",
      "Average Validation Loss: 0.002\n",
      "Epoch 13\tBatch: 1.000, Loss: 0.024, Acc:98.975\n",
      "Average Accuracy: 96.667\n",
      "\n",
      "Average Validation Loss: 0.002\n",
      "Epoch 14\tBatch: 1.000, Loss: 0.034, Acc:98.566\n",
      "Average Accuracy: 96.500\n",
      "\n",
      "Average Validation Loss: 0.003\n",
      "Epoch 15\tBatch: 1.000, Loss: 0.096, Acc:96.107\n",
      "Average Accuracy: 91.667\n",
      "\n",
      "Average Validation Loss: 0.005\n",
      "Epoch 16\tBatch: 1.000, Loss: 0.169, Acc:92.418\n",
      "Average Accuracy: 95.667\n",
      "\n",
      "Average Validation Loss: 0.003\n",
      "Epoch 17\tBatch: 1.000, Loss: 0.040, Acc:99.180\n",
      "Average Accuracy: 98.167\n",
      "\n",
      "Average Validation Loss: 0.001\n",
      "Epoch 18\tBatch: 1.000, Loss: 0.016, Acc:99.385\n",
      "Average Accuracy: 99.000\n",
      "\n",
      "Average Validation Loss: 0.001\n",
      "Epoch 19\tBatch: 1.000, Loss: 0.004, Acc:100.000\n",
      "Average Accuracy: 99.833\n",
      "\n",
      "Average Validation Loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "test_finetune(test_model, extloader, evalloader)\n",
    "# test(test_model, evalloader)\n",
    "test_model.revert_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "9d4eb88f-5c0d-4854-b276-47062b0d4e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Accuracy: 90.833\n",
      "\n",
      "Average Validation Loss: 0.007\n"
     ]
    }
   ],
   "source": [
    "test(test_model, evalloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "5b3facd6-93c9-4e39-9d77-8f572df48db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = torch.randperm(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb66179-79b3-4a37-a660-2e8b19c41b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0227, -0.2342, -0.3712,  ...,  1.3413,  1.3070,  1.3413],\n",
      "         [ 0.3138,  0.3823,  0.8961,  ...,  1.2214,  1.2728,  1.3755],\n",
      "         [ 0.9646,  0.3309,  0.5536,  ..., -0.0287,  0.2624,  0.5878],\n",
      "         ...,\n",
      "         [-0.3369, -0.3883, -0.3883,  ...,  0.0398,  0.0398,  0.0227],\n",
      "         [-0.3369, -0.3198, -0.2513,  ...,  0.2453,  0.2282,  0.2282],\n",
      "         [-0.3198, -0.2684, -0.1486,  ...,  0.3309,  0.3309,  0.3481]],\n",
      "\n",
      "        [[ 0.1527, -0.2675, -0.4601,  ...,  1.5882,  1.5532,  1.6057],\n",
      "         [ 0.5203,  0.4853,  0.9755,  ...,  1.4832,  1.5532,  1.6583],\n",
      "         [ 1.2031,  0.5203,  0.7129,  ..., -0.0924,  0.2052,  0.5378],\n",
      "         ...,\n",
      "         [-0.6001, -0.6352, -0.6176,  ..., -0.2325, -0.2325, -0.2675],\n",
      "         [-0.6176, -0.5651, -0.4951,  ..., -0.0574, -0.0574, -0.0749],\n",
      "         [-0.5826, -0.5126, -0.3901,  ...,  0.0126, -0.0049, -0.0049]],\n",
      "\n",
      "        [[ 0.2173, -0.2184, -0.3578,  ...,  1.9254,  1.8905,  1.9254],\n",
      "         [ 0.6705,  0.6182,  1.1585,  ...,  1.7685,  1.8383,  1.9428],\n",
      "         [ 1.3851,  0.7402,  0.9319,  ...,  0.1651,  0.4788,  0.8099],\n",
      "         ...,\n",
      "         [-1.0898, -1.1247, -1.1421,  ..., -0.6367, -0.6018, -0.6193],\n",
      "         [-1.1073, -1.0898, -1.0550,  ..., -0.4450, -0.4101, -0.3927],\n",
      "         [-1.0724, -1.0201, -0.9330,  ..., -0.3753, -0.3404, -0.3055]]])\n",
      "tensor([[[-0.2342,  0.3138,  0.4679,  ..., -1.4500, -1.4329, -1.4672],\n",
      "         [-0.2513,  0.2967,  0.4851,  ..., -1.5014, -1.4329, -1.5014],\n",
      "         [-0.3198,  0.3138,  0.4679,  ..., -1.4500, -1.3644, -1.4500],\n",
      "         ...,\n",
      "         [-1.0562, -1.4500, -1.0904,  ..., -1.3987, -1.3302, -1.3130],\n",
      "         [-1.0562, -1.2274, -1.1247,  ..., -1.3815, -1.3130, -1.2788],\n",
      "         [-1.1760, -1.0904, -1.1589,  ..., -1.3473, -1.3130, -1.2445]],\n",
      "\n",
      "        [[-0.2150,  0.3277,  0.5203,  ..., -1.3529, -1.3354, -1.3704],\n",
      "         [-0.2325,  0.2927,  0.5378,  ..., -1.4055, -1.3354, -1.4055],\n",
      "         [-0.3200,  0.2927,  0.5028,  ..., -1.3529, -1.2654, -1.3529],\n",
      "         ...,\n",
      "         [-1.0028, -1.3704, -0.9153,  ..., -1.2654, -1.2654, -1.2654],\n",
      "         [-1.0378, -1.1429, -0.9678,  ..., -1.2304, -1.2479, -1.2304],\n",
      "         [-1.1429, -0.9853, -0.9853,  ..., -1.1954, -1.2304, -1.1954]],\n",
      "\n",
      "        [[-0.1138,  0.2522,  0.2871,  ..., -1.1247, -1.1073, -1.1421],\n",
      "         [-0.1487,  0.1999,  0.2696,  ..., -1.1596, -1.1073, -1.1770],\n",
      "         [-0.2358,  0.1825,  0.2348,  ..., -1.0898, -1.0201, -1.1073],\n",
      "         ...,\n",
      "         [-0.7587, -0.9504, -0.3927,  ..., -0.8284, -0.8458, -0.8458],\n",
      "         [-0.8284, -0.7413, -0.4450,  ..., -0.7936, -0.7936, -0.7587],\n",
      "         [-0.9330, -0.5670, -0.4450,  ..., -0.7587, -0.7587, -0.7238]]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(extloader):\n",
    "    x, y = batch[0], batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e238dde-0f0e-4b21-b5a5-f4b06b087783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa03a20-aba1-469a-a37d-96f37f9468f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
